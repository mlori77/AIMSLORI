---
title: "Spray Cooling Prediction by ML and DL"
author: "M. S. Lori"
date: "2023-11-02"
categories: [Paper]
image: "mtheme.jpg"
---

In this post, we have applied some ML and DL algotihms on a dataset compiled from previous studies in the literature about spray cooling systems. The target variable is Nusselt number (Nu). The applying methods are 1- **Random Forest Regression (RFR)** 2- **Support Vector Regression (SVR)** 3- **Decision Tree Regression (DTR)** 4- **XGBoost** 5- **LightGBM** and 6- **Multilayer Perception (MLP)**

1- **Random Forest Regression**

[Importing needed libraries:]{.underline}

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score
```

[Loading dataset:]{.underline}

``` python
url = 'https://raw.githubusercontent.com/mlori77/ANN/main/heatml.csv'
data = pd.read_csv(url)
```

[Encoding:]{.underline}

``` python
data.columns = [a.strip() for a in column]

fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
```

[Correlation matrix:]{.underline}

``` python
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

corr_data = data.copy()

correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True,fmt=".2f", cmap="Greens")
plt.show()
```

[Randomly splitting the dataset into training and testing subsets]{.underline}

``` python
Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'k', 'Nu'], inplace = True, axis = 1)

X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)

y_test.columns = ['y_test']
```

Model definition, Hyperparameters tuning and Prediction

``` python
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state = 42)
from pprint import pprint

print('Parameters currently in use:\n')
pprint(rf.get_params())


from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}




rf = RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(X_train, y_train)

rf_random.best_params_


predict = rf_random.predict(X_test)
```

2- **Support Vector Regression**

[Importing needed libraries:]{.underline}

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
```

[Loading dataset:]{.underline}

``` python
data = pd.read_csv("heatml.csv")
```

[Encoding:]{.underline}

``` python
column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)
data.drop(['Fluid', 'SAT'], inplace=True, axis=1)
```

[Correlation matrix:]{.underline}

``` python

correlation_matrix = corr_data.corr()
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()
```

[Randomly splitting the dataset into training and testing subsets and Scaling]{.underline}

``` python
Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

Model definition, Hyperparameters tuning and Prediction

``` python

svm = SVR()


print('Parameters currently in use:\n')
pprint(svm.get_params())


C = [0.01, 0.1, 1, 10, 100, 1000]
epsilon = [0.01, 0.05, 0.1, 0.2, 0.5]
kernel = ['rbf', 'poly', 'sigmoid']
gamma = ['scale', 'auto', 0.001, 0.01, 0.1]


random_grid = {
    'C': C,
    'epsilon': epsilon,
    'kernel': kernel,
    'gamma': gamma
}
pprint(random_grid)


svm = SVR()


svm_random = RandomizedSearchCV(
    estimator=svm,
    param_distributions=random_grid,
    n_iter=100,
    cv=5,  
    verbose=2,
    random_state=42,
    n_jobs=-1
)


svm_random.fit(X_train, y_train)


svm_random.best_params_


predict = svm_random.predict(X_test)  
```

3- **Decision Tree**

[Importing needed libraries:]{.underline}

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
```

[Loading dataset:]{.underline}

``` python
data = pd.read_csv("heatml.csv")
```

[Encoding:]{.underline}

``` python
column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)
```

[Correlation matrix:]{.underline}

``` python
corr_data = data.copy()

correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()
```

[Randomly splitting the dataset into training and testing subsets]{.underline}

``` python
Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)

X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)
y_test.columns = ['y_test']
```

Model definition, Hyperparameters tuning and Prediction

``` python

dt = DecisionTreeRegressor(random_state=42)



max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)

min_samples_split = [2, 5, 10]


min_samples_leaf = [1, 2, 4]


max_features = ['auto', 'sqrt', None]


criterion = ['mse', 'friedman_mse', 'mae']


random_grid = {
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'max_features': max_features,
    'criterion': criterion
}




dt = DecisionTreeRegressor()


dt_random = RandomizedSearchCV(
    estimator=dt,
    param_distributions=random_grid,
    n_iter=100,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

dt_random.fit(X_train, y_train)


dt_random.best_params_


predict = dt_random.predict(X_test)
```

4- **XGBoost**

[Importing and installing needed libraries:]{.underline}

``` python
!pip install xgboost
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
```

[Loading dataset:]{.underline}

``` python
data = pd.read_csv("heatml.csv")
```

[Encoding:]{.underline}

``` python
column = data.columns.to_list()
data.columns = [a.strip() for a in column]

fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)
```

[Correlation matrix:]{.underline}

``` python
corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()
```

[Randomly splitting the dataset into training and testing subsets]{.underline}

``` python
Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']
```

Model definition, Hyperparameters tuning and Prediction

``` python
xg_reg = XGBRegressor(random_state=42)


param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'min_child_weight': [1, 5, 10]
}


pprint(param_dist)


random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, 
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)
```

5- **LightGBM**

[Importing and installing needed libraries:]{.underline}

``` python
!pip install lightgbm
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
```

[Loading dataset:]{.underline}

``` python
data = pd.read_csv("heatml.csv")
```

[Encoding:]{.underline}

``` python

column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)
```

[Correlation matrix:]{.underline}

``` python

corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()
```

[Randomly splitting the dataset into training and testing subsets]{.underline}

``` python

Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']
```

Model definition, Hyperparameters tuning and Prediction

``` python

lgb_reg = lgb.LGBMRegressor(random_state=42)


param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'min_child_samples': [5, 10, 20],
    'reg_alpha': [0, 0.1, 0.2, 0.3],
    'reg_lambda': [0, 0.1, 0.2, 0.3]
}





random_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)
```

\
**Prediction on Gas Atomizer Sprays**

1- **RFR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score


data = pd.read_csv("heatml-gas.csv")

column = data.columns.to_list()


data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)


data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

corr_data = data.copy()


correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True,fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)

y_test.columns = ['y_test']

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state = 42)
from pprint import pprint

pprint(rf.get_params())


from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}




rf = RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)

rf_random.fit(X_train, y_train)

rf_random.best_params_


predict = rf_random.predict(X_test)



yt = y_test.to_numpy()


predict.reshape(6,1)

result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)

result.to_csv('rfrnuhtgas.csv')
```

2- **SVR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-gas.csv")

column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)
data.drop(['Fluid', 'SAT'], inplace=True, axis=1)

corr_data = data.copy()


correlation_matrix = corr_data.corr()
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


svm = SVR()






C = [0.01, 0.1, 1, 10, 100, 1000]
epsilon = [0.01, 0.05, 0.1, 0.2, 0.5]
kernel = ['rbf', 'poly', 'sigmoid']
gamma = ['scale', 'auto', 0.001, 0.01, 0.1]


random_grid = {
    'C': C,
    'epsilon': epsilon,
    'kernel': kernel,
    'gamma': gamma
}
pprint(random_grid)


svm = SVR()


svm_random = RandomizedSearchCV(
    estimator=svm,
    param_distributions=random_grid,
    n_iter=100,
    cv=5,  
    verbose=2,
    random_state=42,
    n_jobs=-1
)


svm_random.fit(X_train, y_train)


svm_random.best_params_


predict = svm_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
result.to_csv('svmnuhtgas.csv')
```

3- **DT**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-gas.csv")

column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

corr_data = data.copy()

correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)

X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)
y_test.columns = ['y_test']


dt = DecisionTreeRegressor(random_state=42)




max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)


min_samples_split = [2, 5, 10]


min_samples_leaf = [1, 2, 4]


max_features = ['auto', 'sqrt', None]


criterion = ['mse', 'friedman_mse', 'mae']


random_grid = {
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'max_features': max_features,
    'criterion': criterion
}



dt = DecisionTreeRegressor()


dt_random = RandomizedSearchCV(
    estimator=dt,
    param_distributions=random_grid,
    n_iter=100,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)


dt_random.fit(X_train, y_train)


dt_random.best_params_


predict = dt_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
result.to_csv('dtnuhtgas.csv')
```

4- **XGBoost**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-gas.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


xg_reg = XGBRegressor(random_state=42)



param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'min_child_weight': [1, 5, 10]
}


pprint(param_dist)


random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, 
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)


yt = y_test.to_numpy()


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)


result.to_csv('xgboostgas.csv')


```

5- **LightBGM**

``` python
!pip install lightgbm
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-gas.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


lgb_reg = lgb.LGBMRegressor(random_state=42)


param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'min_child_samples': [5, 10, 20],
    'reg_alpha': [0, 0.1, 0.2, 0.3],
    'reg_lambda': [0, 0.1, 0.2, 0.3]
}


pprint(param_dist)


random_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)


yt = y_test.to_numpy()


lgbm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)


lgbm_result.to_csv('lgbmgas_results.csv')
```

**Prediction on Pressure Atomizer Sprays**

1- **RFR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score


data = pd.read_csv("heatml-pressure.csv")

column = data.columns.to_list()


data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)


data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

corr_data = data.copy()


correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True,fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)

y_test.columns = ['y_test']

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state = 42)
from pprint import pprint

pprint(rf.get_params())


from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}




rf = RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)

rf_random.fit(X_train, y_train)

rf_random.best_params_


predict = rf_random.predict(X_test)



yt = y_test.to_numpy()


predict.reshape(101,1)

result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)

result.to_csv('rfrnuhtpressure.csv')
```

2- **SVR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-pressure.csv")

column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)
data.drop(['Fluid', 'SAT'], inplace=True, axis=1)

corr_data = data.copy()


correlation_matrix = corr_data.corr()
plt.figure(figsize=(16, 12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


svm = SVR()




C = [0.01, 0.1, 1, 10, 100, 1000]
epsilon = [0.01, 0.05, 0.1, 0.2, 0.5]
kernel = ['rbf', 'poly', 'sigmoid']
gamma = ['scale', 'auto', 0.001, 0.01, 0.1]


random_grid = {
    'C': C,
    'epsilon': epsilon,
    'kernel': kernel,
    'gamma': gamma
}
pprint(random_grid)


svm = SVR()


svm_random = RandomizedSearchCV(
    estimator=svm,
    param_distributions=random_grid,
    n_iter=100,
    cv=5,  
    verbose=2,
    random_state=42,
    n_jobs=-1
)


svm_random.fit(X_train, y_train)


svm_random.best_params_


predict = svm_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


svm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
svm_result.to_csv('svmnuhtpressure_svm.csv')
```

3- **DT**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-pressure.csv")

column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

corr_data = data.copy()

correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)

X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)
y_test.columns = ['y_test']


dt = DecisionTreeRegressor(random_state=42)




max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)


min_samples_split = [2, 5, 10]


min_samples_leaf = [1, 2, 4]


max_features = ['auto', 'sqrt', None]


criterion = ['mse', 'friedman_mse', 'mae']


random_grid = {
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'max_features': max_features,
    'criterion': criterion
}
pprint(random_grid)


dt = DecisionTreeRegressor()


dt_random = RandomizedSearchCV(
    estimator=dt,
    param_distributions=random_grid,
    n_iter=100,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)


dt_random.fit(X_train, y_train)


dt_random.best_params_


predict = dt_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
result.to_csv('dtnuhtpressure.csv')
```

4- **XGBoost**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-pressure.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


xg_reg = XGBRegressor(random_state=42)



param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'min_child_weight': [1, 5, 10]
}




random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, 
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)


yt = y_test.to_numpy()


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)


result.to_csv('xgboostpressure.csv')
```

5- **LightGBM**

``` python
!pip install lightgbm
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("heatml-pressure.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)

data.drop(['Fluid', 'SAT'], inplace=True, axis=1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()

plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()


Y = pd.DataFrame(data['Nu'], columns=['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)


X_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)
y_test.columns = ['y_test']


lgb_reg = lgb.LGBMRegressor(random_state=42)



param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'min_child_samples': [5, 10, 20],
    'reg_alpha': [0, 0.1, 0.2, 0.3],
    'reg_lambda': [0, 0.1, 0.2, 0.3]
}




random_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


print("Best hyperparameters found: ", random_search.best_params_)


predict = random_search.predict(X_test)
```

**Testing on an excluded subset**

1- **RFR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score



data = pd.read_csv("WholeExcludeCiofalo.csv")


data1 = pd.read_csv("Ciofalo.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


column1 = data1.columns.to_list()
data1.columns = [a.strip() for a in column1]


fluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)
data1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

Y_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])
data1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train = data
y_train = Y
X_test = data1
y_test = Y_test






from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state = 42)
from pprint import pprint




from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]

max_features = ['auto', 'sqrt']

max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)

min_samples_split = [2, 5, 10]

min_samples_leaf = [1, 2, 4]

bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}




rf = RandomForestRegressor()


rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(X_train, y_train)

rf_random.best_params_


predict = rf_random.predict(X_test)



yt = y_test.to_numpy()


predict.reshape(63,1)

result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)

result.to_csv('rfrnuhtexc.csv')
```

2- **SVR**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint



data = pd.read_csv("WholeExcludeCiofalo.csv")


data1 = pd.read_csv("Ciofalo.csv")



column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


column1 = data1.columns.to_list()
data1.columns = [a.strip() for a in column1]


fluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)
data1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

Y_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])
data1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train = data
y_train = Y
X_test = data1
y_test = Y_test





scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


svm = SVR()





C = [0.01, 0.1, 1, 10, 100, 1000]
epsilon = [0.01, 0.05, 0.1, 0.2, 0.5]
kernel = ['rbf', 'poly', 'sigmoid']
gamma = ['scale', 'auto', 0.001, 0.01, 0.1]


random_grid = {
    'C': C,
    'epsilon': epsilon,
    'kernel': kernel,
    'gamma': gamma
}
pprint(random_grid)



svm = SVR()


svm_random = RandomizedSearchCV(
    estimator=svm,
    param_distributions=random_grid,
    n_iter=100,
    cv=5,  
    verbose=2,
    random_state=42,
    n_jobs=-1
)


svm_random.fit(X_train, y_train)


svm_random.best_params_


predict = svm_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


svm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
svm_result.to_csv('svmnuhtexcluded.csv')
```

3- **DT**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint

data = pd.read_csv("WholeExcludeCiofalo.csv")


data1 = pd.read_csv("Ciofalo.csv")


column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


column1 = data1.columns.to_list()
data1.columns = [a.strip() for a in column1]


fluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)
data1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

Y_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])
data1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train = data
y_train = Y
X_test = data1
y_test = Y_test


dt = DecisionTreeRegressor(random_state=42)


max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
max_features = ['auto', 'sqrt', None]
criterion = ['mse', 'friedman_mse', 'mae']


random_grid = {
    'max_depth': max_depth,
    'min_samples_split': min_samples_split,
    'min_samples_leaf': min_samples_leaf,
    'max_features': max_features,
    'criterion': criterion
}



dt_random = RandomizedSearchCV(
    estimator=dt,
    param_distributions=random_grid,
    n_iter=100,
    cv=3,
    verbose=2,
    random_state=42,
    n_jobs=-1
)


dt_random.fit(X_train, y_train)


dt_random.best_params_


predict = dt_random.predict(X_test)


yt = y_test.to_numpy()


predict = predict.reshape(-1, 1)


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)
result.to_csv('dtnuhtexc.csv')
```

4- **XGBoost**

``` python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint



data = pd.read_csv("WholeExcludeCiofalo.csv")


data1 = pd.read_csv("Ciofalo.csv")



column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


column1 = data1.columns.to_list()
data1.columns = [a.strip() for a in column1]



fluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)
data1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

Y_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])
data1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train = data
y_train = Y
X_test = data1
y_test = Y_test





xg_reg = XGBRegressor(random_state=42)


param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'min_child_weight': [1, 5, 10]
}



random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, 
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)


yt = y_test.to_numpy()


result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)


result.to_csv('xgboosexc.csv')
```

5- **LightGBM**

``` python
!pip install lightgbm
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint


data = pd.read_csv("WholeExcludeCiofalo.csv")


data1 = pd.read_csv("Ciofalo.csv")



column = data.columns.to_list()
data.columns = [a.strip() for a in column]


fluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)
data.drop(['Fluid', 'SAT'], inplace = True, axis = 1)


corr_data = data.copy()
correlation_matrix = corr_data.corr()
plt.figure(figsize=(16,12))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="Greens")
plt.show()

Y = pd.DataFrame(data['Nu'], columns = ['Nu'])
data.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


column1 = data1.columns.to_list()
data1.columns = [a.strip() for a in column1]


fluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)
data1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)

Y_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])
data1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)


X_train = data
y_train = Y
X_test = data1
y_test = Y_test






lgb_reg = lgb.LGBMRegressor(random_state=42)



param_dist = {
    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],
    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],
    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'min_child_samples': [5, 10, 20],
    'reg_alpha': [0, 0.1, 0.2, 0.3],
    'reg_lambda': [0, 0.1, 0.2, 0.3]
}





random_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,
                                   cv=3, verbose=2, random_state=42, n_jobs=-1)


random_search.fit(X_train, y_train)


random_search.best_params_


predict = random_search.predict(X_test)


yt = y_test.to_numpy()


lgbm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)


lgbm_result.to_csv('lgbmexcluded.csv')
```
