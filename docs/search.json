[
  {
    "objectID": "posts/Spray Cooling Systems/index.html",
    "href": "posts/Spray Cooling Systems/index.html",
    "title": "Spray Cooling Prediction by ML and DL",
    "section": "",
    "text": "In this post, we have applied some ML and DL algotihms on a dataset compiled from previous studies in the literature about spray cooling systems. The target variable is Nusselt number (Nu). The applying methods are 1- Random Forest Regression (RFR) 2- Support Vector Regression (SVR) 3- Decision Tree Regression (DTR) 4- XGBoost 5- LightGBM and 6- Multilayer Perception (MLP)\n1- Random Forest Regression\nImporting needed libraries:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nLoading dataset:\nurl = 'https://raw.githubusercontent.com/mlori77/ANN/main/heatml.csv'\ndata = pd.read_csv(url)\nEncoding:\ndata.columns = [a.strip() for a in column]\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\nCorrelation matrix:\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nRandomly splitting the dataset into training and testing subsets\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'k', 'Nu'], inplace = True, axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\n\ny_test.columns = ['y_test']\nModel definition, Hyperparameters tuning and Prediction\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\nmin_samples_leaf = [1, 2, 4]\n\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrf = RandomForestRegressor()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_\n\n\npredict = rf_random.predict(X_test)\n2- Support Vector Regression\nImporting needed libraries:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nLoading dataset:\ndata = pd.read_csv(\"heatml.csv\")\nEncoding:\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\nCorrelation matrix:\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nRandomly splitting the dataset into training and testing subsets and Scaling\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nModel definition, Hyperparameters tuning and Prediction\n\nsvm = SVR()\n\n\nprint('Parameters currently in use:\\n')\npprint(svm.get_params())\n\n\nC = [0.01, 0.1, 1, 10, 100, 1000]\nepsilon = [0.01, 0.05, 0.1, 0.2, 0.5]\nkernel = ['rbf', 'poly', 'sigmoid']\ngamma = ['scale', 'auto', 0.001, 0.01, 0.1]\n\n\nrandom_grid = {\n    'C': C,\n    'epsilon': epsilon,\n    'kernel': kernel,\n    'gamma': gamma\n}\npprint(random_grid)\n\n\nsvm = SVR()\n\n\nsvm_random = RandomizedSearchCV(\n    estimator=svm,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=5,  \n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\nsvm_random.fit(X_train, y_train)\n\n\nsvm_random.best_params_\n\n\npredict = svm_random.predict(X_test)  \n3- Decision Tree\nImporting needed libraries:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nLoading dataset:\ndata = pd.read_csv(\"heatml.csv\")\nEncoding:\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\nCorrelation matrix:\ncorr_data = data.copy()\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nRandomly splitting the dataset into training and testing subsets\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\ny_test.columns = ['y_test']\nModel definition, Hyperparameters tuning and Prediction\n\ndt = DecisionTreeRegressor(random_state=42)\n\n\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\n\nmin_samples_leaf = [1, 2, 4]\n\n\nmax_features = ['auto', 'sqrt', None]\n\n\ncriterion = ['mse', 'friedman_mse', 'mae']\n\n\nrandom_grid = {\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf,\n    'max_features': max_features,\n    'criterion': criterion\n}\n\n\n\n\ndt = DecisionTreeRegressor()\n\n\ndt_random = RandomizedSearchCV(\n    estimator=dt,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\ndt_random.fit(X_train, y_train)\n\n\ndt_random.best_params_\n\n\npredict = dt_random.predict(X_test)\n4- XGBoost\nImporting and installing needed libraries:\n!pip install xgboost\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nLoading dataset:\ndata = pd.read_csv(\"heatml.csv\")\nEncoding:\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\nCorrelation matrix:\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nRandomly splitting the dataset into training and testing subsets\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\nModel definition, Hyperparameters tuning and Prediction\nxg_reg = XGBRegressor(random_state=42)\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 5, 10]\n}\n\n\npprint(param_dist)\n\n\nrandom_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, \n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n5- LightGBM\nImporting and installing needed libraries:\n!pip install lightgbm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nLoading dataset:\ndata = pd.read_csv(\"heatml.csv\")\nEncoding:\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\nCorrelation matrix:\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nRandomly splitting the dataset into training and testing subsets\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\nModel definition, Hyperparameters tuning and Prediction\n\nlgb_reg = lgb.LGBMRegressor(random_state=42)\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'min_child_samples': [5, 10, 20],\n    'reg_alpha': [0, 0.1, 0.2, 0.3],\n    'reg_lambda': [0, 0.1, 0.2, 0.3]\n}\n\n\n\n\n\nrandom_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,\n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n\nPrediction on Gas Atomizer Sprays\n1- RFR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\n\n\ndata = pd.read_csv(\"heatml-gas.csv\")\n\ncolumn = data.columns.to_list()\n\n\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\n\n\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\n\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\n\ny_test.columns = ['y_test']\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n\npprint(rf.get_params())\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\nmin_samples_leaf = [1, 2, 4]\n\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrf = RandomForestRegressor()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_\n\n\npredict = rf_random.predict(X_test)\n\n\n\nyt = y_test.to_numpy()\n\n\npredict.reshape(6,1)\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)\n\nresult.to_csv('rfrnuhtgas.csv')\n2- SVR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-gas.csv\")\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\ncorr_data = data.copy()\n\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nsvm = SVR()\n\n\n\n\n\n\nC = [0.01, 0.1, 1, 10, 100, 1000]\nepsilon = [0.01, 0.05, 0.1, 0.2, 0.5]\nkernel = ['rbf', 'poly', 'sigmoid']\ngamma = ['scale', 'auto', 0.001, 0.01, 0.1]\n\n\nrandom_grid = {\n    'C': C,\n    'epsilon': epsilon,\n    'kernel': kernel,\n    'gamma': gamma\n}\npprint(random_grid)\n\n\nsvm = SVR()\n\n\nsvm_random = RandomizedSearchCV(\n    estimator=svm,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=5,  \n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\nsvm_random.fit(X_train, y_train)\n\n\nsvm_random.best_params_\n\n\npredict = svm_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nresult.to_csv('svmnuhtgas.csv')\n3- DT\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-gas.csv\")\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\ny_test.columns = ['y_test']\n\n\ndt = DecisionTreeRegressor(random_state=42)\n\n\n\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n\n\nmin_samples_split = [2, 5, 10]\n\n\nmin_samples_leaf = [1, 2, 4]\n\n\nmax_features = ['auto', 'sqrt', None]\n\n\ncriterion = ['mse', 'friedman_mse', 'mae']\n\n\nrandom_grid = {\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf,\n    'max_features': max_features,\n    'criterion': criterion\n}\n\n\n\ndt = DecisionTreeRegressor()\n\n\ndt_random = RandomizedSearchCV(\n    estimator=dt,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\ndt_random.fit(X_train, y_train)\n\n\ndt_random.best_params_\n\n\npredict = dt_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nresult.to_csv('dtnuhtgas.csv')\n4- XGBoost\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-gas.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nxg_reg = XGBRegressor(random_state=42)\n\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 5, 10]\n}\n\n\npprint(param_dist)\n\n\nrandom_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, \n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\n\n\nresult.to_csv('xgboostgas.csv')\n\n5- LightBGM\n!pip install lightgbm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-gas.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nlgb_reg = lgb.LGBMRegressor(random_state=42)\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'min_child_samples': [5, 10, 20],\n    'reg_alpha': [0, 0.1, 0.2, 0.3],\n    'reg_lambda': [0, 0.1, 0.2, 0.3]\n}\n\n\npprint(param_dist)\n\n\nrandom_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,\n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\nlgbm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\n\n\nlgbm_result.to_csv('lgbmgas_results.csv')\nPrediction on Pressure Atomizer Sprays\n1- RFR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\n\n\ndata = pd.read_csv(\"heatml-pressure.csv\")\n\ncolumn = data.columns.to_list()\n\n\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\n\n\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\n\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\n\ny_test.columns = ['y_test']\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n\npprint(rf.get_params())\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\nmin_samples_leaf = [1, 2, 4]\n\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrf = RandomForestRegressor()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_\n\n\npredict = rf_random.predict(X_test)\n\n\n\nyt = y_test.to_numpy()\n\n\npredict.reshape(101,1)\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)\n\nresult.to_csv('rfrnuhtpressure.csv')\n2- SVR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-pressure.csv\")\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\ncorr_data = data.copy()\n\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16, 12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nsvm = SVR()\n\n\n\n\nC = [0.01, 0.1, 1, 10, 100, 1000]\nepsilon = [0.01, 0.05, 0.1, 0.2, 0.5]\nkernel = ['rbf', 'poly', 'sigmoid']\ngamma = ['scale', 'auto', 0.001, 0.01, 0.1]\n\n\nrandom_grid = {\n    'C': C,\n    'epsilon': epsilon,\n    'kernel': kernel,\n    'gamma': gamma\n}\npprint(random_grid)\n\n\nsvm = SVR()\n\n\nsvm_random = RandomizedSearchCV(\n    estimator=svm,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=5,  \n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\nsvm_random.fit(X_train, y_train)\n\n\nsvm_random.best_params_\n\n\npredict = svm_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nsvm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nsvm_result.to_csv('svmnuhtpressure_svm.csv')\n3- DT\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-pressure.csv\")\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size = 0.2, random_state = 0)\ny_test.columns = ['y_test']\n\n\ndt = DecisionTreeRegressor(random_state=42)\n\n\n\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n\n\nmin_samples_split = [2, 5, 10]\n\n\nmin_samples_leaf = [1, 2, 4]\n\n\nmax_features = ['auto', 'sqrt', None]\n\n\ncriterion = ['mse', 'friedman_mse', 'mae']\n\n\nrandom_grid = {\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf,\n    'max_features': max_features,\n    'criterion': criterion\n}\npprint(random_grid)\n\n\ndt = DecisionTreeRegressor()\n\n\ndt_random = RandomizedSearchCV(\n    estimator=dt,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\ndt_random.fit(X_train, y_train)\n\n\ndt_random.best_params_\n\n\npredict = dt_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nresult.to_csv('dtnuhtpressure.csv')\n4- XGBoost\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-pressure.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nxg_reg = XGBRegressor(random_state=42)\n\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 5, 10]\n}\n\n\n\n\nrandom_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, \n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\n\n\nresult.to_csv('xgboostpressure.csv')\n5- LightGBM\n!pip install lightgbm\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\ndata = pd.read_csv(\"heatml-pressure.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first=True)\n\ndata.drop(['Fluid', 'SAT'], inplace=True, axis=1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\nY = pd.DataFrame(data['Nu'], columns=['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace=True, axis=1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.2, random_state=0)\ny_test.columns = ['y_test']\n\n\nlgb_reg = lgb.LGBMRegressor(random_state=42)\n\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'min_child_samples': [5, 10, 20],\n    'reg_alpha': [0, 0.1, 0.2, 0.3],\n    'reg_lambda': [0, 0.1, 0.2, 0.3]\n}\n\n\n\n\nrandom_search = RandomizedSearchCV(estimator=lgb_reg, param_distributions=param_dist, n_iter=100,\n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nprint(\"Best hyperparameters found: \", random_search.best_params_)\n\n\npredict = random_search.predict(X_test)\nTesting on an excluded subset\n1- RFR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\n\n\n\ndata = pd.read_csv(\"WholeExcludeCiofalo.csv\")\n\n\ndata1 = pd.read_csv(\"Ciofalo.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\ncolumn1 = data1.columns.to_list()\ndata1.columns = [a.strip() for a in column1]\n\n\nfluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)\ndata1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\nY_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])\ndata1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train = data\ny_train = Y\nX_test = data1\ny_test = Y_test\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n\n\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\nmin_samples_split = [2, 5, 10]\n\nmin_samples_leaf = [1, 2, 4]\n\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n\n\n\nrf = RandomForestRegressor()\n\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\n\nrf_random.best_params_\n\n\npredict = rf_random.predict(X_test)\n\n\n\nyt = y_test.to_numpy()\n\n\npredict.reshape(63,1)\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis = 1)\n\nresult.to_csv('rfrnuhtexc.csv')\n2- SVR\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\n\ndata = pd.read_csv(\"WholeExcludeCiofalo.csv\")\n\n\ndata1 = pd.read_csv(\"Ciofalo.csv\")\n\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\ncolumn1 = data1.columns.to_list()\ndata1.columns = [a.strip() for a in column1]\n\n\nfluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)\ndata1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\nY_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])\ndata1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train = data\ny_train = Y\nX_test = data1\ny_test = Y_test\n\n\n\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nsvm = SVR()\n\n\n\n\n\nC = [0.01, 0.1, 1, 10, 100, 1000]\nepsilon = [0.01, 0.05, 0.1, 0.2, 0.5]\nkernel = ['rbf', 'poly', 'sigmoid']\ngamma = ['scale', 'auto', 0.001, 0.01, 0.1]\n\n\nrandom_grid = {\n    'C': C,\n    'epsilon': epsilon,\n    'kernel': kernel,\n    'gamma': gamma\n}\npprint(random_grid)\n\n\n\nsvm = SVR()\n\n\nsvm_random = RandomizedSearchCV(\n    estimator=svm,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=5,  \n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\nsvm_random.fit(X_train, y_train)\n\n\nsvm_random.best_params_\n\n\npredict = svm_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nsvm_result = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nsvm_result.to_csv('svmnuhtexcluded.csv')\n3- DT\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\ndata = pd.read_csv(\"WholeExcludeCiofalo.csv\")\n\n\ndata1 = pd.read_csv(\"Ciofalo.csv\")\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\ncolumn1 = data1.columns.to_list()\ndata1.columns = [a.strip() for a in column1]\n\n\nfluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)\ndata1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\nY_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])\ndata1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train = data\ny_train = Y\nX_test = data1\ny_test = Y_test\n\n\ndt = DecisionTreeRegressor(random_state=42)\n\n\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nmax_features = ['auto', 'sqrt', None]\ncriterion = ['mse', 'friedman_mse', 'mae']\n\n\nrandom_grid = {\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf,\n    'max_features': max_features,\n    'criterion': criterion\n}\n\n\n\ndt_random = RandomizedSearchCV(\n    estimator=dt,\n    param_distributions=random_grid,\n    n_iter=100,\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\n\ndt_random.fit(X_train, y_train)\n\n\ndt_random.best_params_\n\n\npredict = dt_random.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\npredict = predict.reshape(-1, 1)\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\nresult.to_csv('dtnuhtexc.csv')\n4- XGBoost\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\n\n\ndata = pd.read_csv(\"WholeExcludeCiofalo.csv\")\n\n\ndata1 = pd.read_csv(\"Ciofalo.csv\")\n\n\n\ncolumn = data.columns.to_list()\ndata.columns = [a.strip() for a in column]\n\n\nfluid_dummy = pd.get_dummies(data['Fluid'], drop_first = True)\ndata.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nY = pd.DataFrame(data['Nu'], columns = ['Nu'])\ndata.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\ncolumn1 = data1.columns.to_list()\ndata1.columns = [a.strip() for a in column1]\n\n\n\nfluid_dummy1 = pd.get_dummies(data1['Fluid'], drop_first = True)\ndata1.drop(['Fluid', 'SAT'], inplace = True, axis = 1)\n\nY_test = pd.DataFrame(data1['Nu'], columns = ['Nu'])\ndata1.drop(['P', 'um', 'Pr', 'rho', 'k', 'Nu', 'At'], inplace = True, axis = 1)\n\n\nX_train = data\ny_train = Y\nX_test = data1\ny_test = Y_test\n\n\n\n\n\nxg_reg = XGBRegressor(random_state=42)\n\n\nparam_dist = {\n    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n    'max_depth': [int(x) for x in np.linspace(3, 15, num=13)],\n    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n    'min_child_weight': [1, 5, 10]\n}\n\n\n\nrandom_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=100, \n                                   cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nrandom_search.best_params_\n\n\npredict = random_search.predict(X_test)\n\n\nyt = y_test.to_numpy()\n\n\nresult = pd.concat([pd.DataFrame(predict), pd.DataFrame(yt)], axis=1)\n\n\nresult.to_csv('xgboosexc.csv')"
  },
  {
    "objectID": "posts/Probability Theory (Naïve Bayes Classification)/index.html",
    "href": "posts/Probability Theory (Naïve Bayes Classification)/index.html",
    "title": "Probability Theory (Naïve Bayes Classification)",
    "section": "",
    "text": "Probability Theory (Naïve Bayes Classification)\nIn this blog, we will first represent an introduction to the Bayes theorem, the foundation of Bayes classifier. Then it is learnt how to create and assess a Naïve Bayes classifier using Python Sklearn module.\nBayes’ theorem is a simple mathematic formula used for calculating conditional probability. Its formula is:\n\nP(A|B): The probability of event A occurring given that B is true (posterior probability of A given B)\nP(B|A): The probability of event A occurring given that B is true (posterior probability of A given B)\nExample: watching movies based on genre\nResults are based as bellow\n\n Frequency and probability table:\n\nCalculating the probability of watching the genre of Drama\nP(Yes|Drama) = P(Drama|Yes) * P(Yes)/P(Drama)\nP(Drama) = 4/14, P(Yes) = 9/14, P(Overcast|Yes) = 4/9\nSo,\nP(Yes|Drama) = 0.98\nSimilarly, not watching the Drama genre\nP(No|Drama) = P(Drama|No) * P(No)/P(Drama)\nP(Drama) = 4/14, P(No) = 5/14, P(Overcast|No) = 0/5\nSo,\nP(No|Drama) = 0\nThe probability of ‘yes’ class is higher. So if the film is in drama genre, it will be watched\nIn case of having multiple characteristics to calculating probability the Bayes’ classifier obey the following steps:\n\nCalculate prior probability for given class table\nCalculate conditional probability with each feature for each class\nMultiply same class conditional probability\nMultiply prior probability with previous step probability\nSee which class has higher probability\n\nFor example if we include the price of movies on our decision to watch it or not (Price = expensive, reasonable and cheap)\nP(Yes | Drama, Reasonable) = P(Drama, Reasonable | Yes) * P(Yes) ….. (For comparison with do not need the denominator)\nP(No | Drama, Reasonable) = (Drama, Reasonable | No) * P(No) ….\nTraining and assessment of Bayes’ Classifier module on artificially manufactured data (Synthetic data)\nData Generation\nArtificial data generation is often useful when there is no real-world data or real information are kept private due to compliance risks.\nSklearn module enable us to generate synthetic information using make_classification function.  Synthetic data are customizable, which means it is possible to create data that meet our needs. In this case, we are begetting a dataset with a desired numbers of classes, features and samples.\n## Generating the Dataset\n\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_features=6,\n    n_classes=3,\n    n_samples=800,\n    n_informative=2,\n    random_state=1,\n    n_clusters_per_class=1,\n)\nCode explanation:\n\nImport the make_classification function from sklearn module\nMake_classification function generate a random dataset for classification projects\nFunction arguments: n_features: number of independent variables; n_class: the number of target variables; n_samples: the number of observations; n-Informative: the number of influential features on target variables; random_state: ensuring that dataset is reproducible; n_clusster_per_class: determine the degree of separation between the classes.  \nX: features of dataset\ny: target variables\n\nVisualization of dataset importing scatter functions from matplotlib module\n## visualize the dataset\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X[:, 0], X[:, 1], c=y, marker=\"*\");\nCode explanation:\n\nimport scatter from matplotlib\nscatter function takes first and second columns of X array; c=y provides colors for each data point; marker assign a shaper for each of data point.\n\n\nAs you can see there are three target labels (Multiclass classification model)\nTrain and test datasets\nA proficient supervised model excels in delivering accurate predictions on new data. The availability of fresh data facilitates the assessment of model performance. Nevertheless, in situations where new information is unavailable, it proves beneficial to partition the existing data into two sets: training and testing.\nThe train-test procedure is spelled out in the bellow figure:\n\n## Train Test Split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=125\n)\nCode explanation:\n\nimport train_test_split function from sklearn module\ntrain_test_split function arguments: X (input features), y (target variables), test_size (proportion of test size) and random_size\n\nIn the next step, we build the Gaussian Naïve Bayes model, train it and make prediction:\nfrom sklearn.naive_bayes import GaussianNB\n\n# Build a Gaussian Classifier\nmodel = GaussianNB()\n\n# Model training\nmodel.fit(X_train, y_train)\nBoth actual and predicted values and the same:\n# Predict Output\npredicted = model.predict([X_test[6]])\n\nprint(\"Actual Value:\", y_test[6])\nprint(\"Predicted Value:\", predicted[0])\n\nModel assessment\nWe make a prediction of test dataset and then calculate the accuracy and F1-score (a criteria for precision and recall):\n\nImporting several functions from the sklearn.metrics module, including accuracy_score, confusion_matrix, ConfusionMatrixDisplay, and f1_score\nMaking prediction is done by model.predict\n\nBased on values for accuracy and f1-score, we concluded our model works properly.\nTrue positive and true negative are calculated by confusion_matrix and visualized by ConfusionMatrixDisplay.\nOur model performed in a good way. However, there are some ways to improve it more like scaling, cross-validation and hyperparameter optimization.\n## Model Evaluation\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n)\n\ny_pred = model.predict(X_test)\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\n\n## visualize the Confusion matrix\n\nlabels = [0,1,2]\ncm = confusion_matrix(y_test, y_pred, labels=labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot();\n\n\nAll codes:\n## Generating the Dataset\n\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_features=6,\n    n_classes=3,\n    n_samples=800,\n    n_informative=2,\n    random_state=1,\n    n_clusters_per_class=1,\n)\n\n## visualize the dataset\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X[:, 0], X[:, 1], c=y, marker=\"*\");\n\n## Train Test Split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=125\n)\n\nfrom sklearn.naive_bayes import GaussianNB\n\n# Build a Gaussian Classifier\nmodel = GaussianNB()\n\n# Model training\nmodel.fit(X_train, y_train)\n\n# Predict Output\npredicted = model.predict([X_test[6]])\n\nprint(\"Actual Value:\", y_test[6])\nprint(\"Predicted Value:\", predicted[0])\n\n## Model Evaluation\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n)\n\ny_pred = model.predict(X_test)\naccuray = accuracy_score(y_pred, y_test)\nf1 = f1_score(y_pred, y_test, average=\"weighted\")\n\nprint(\"Accuracy:\", accuray)\nprint(\"F1 Score:\", f1)\n\n## visualize the Confusion matrix\n\nlabels = [0,1,2]\ncm = confusion_matrix(y_test, y_pred, labels=labels)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot();"
  },
  {
    "objectID": "posts/implement different classification modules/index.html",
    "href": "posts/implement different classification modules/index.html",
    "title": "Implement Different Classification Models",
    "section": "",
    "text": "In this post, we try to apply different classification models on a real data. In addition, we provide tables and diagrams for optimized hyperparameters.\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\nFirst we import required and utilized model libraries:\n# Importing\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import RFECV\nLoading data and having and general view on them:\n# loading data\n\ndata = pd.read_csv('fetal_health.csv')\nHistogram diagram of a few of data:\n\n\n\n\nReview on data: Some of the data exhibit a normal distribution, while others show a skewed distribution. The “fetal_health” and “histogram_tendency” columns contain categorical variables. The data is imbalanced, with the majority of fetal assessments being positive. Additionally, there are significant variations in the minimum and maximum values of the features.\nExamining the relationship among different features:\n# checking connection between different features\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\nCertain features exhibit strong positive and negative correlations, necessitating the need for feature selection.\nTrain and Test Split with test size of 20%:\n# test and train split\n\ny = data[['fetal_health']]\nfeatures = data.drop(['fetal_health'], axis=1)\nx = (features - features.mean()) / (features.std()) #standardized features\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\nRecursive Feature Elimination: Recursive Feature Elimination (RFE) is a technique for feature selection wherein a model is fitted, and the least impactful feature (or features) is iteratively removed until the desired number of features is attained. The ranking of features is determined by the model’s coef_ or feature_importances_ attributes. By systematically eliminating a small number of features in each iteration, RFE aims to mitigate dependencies and collinearity (a situation where two or more predictor variables are closely related to one another) within the model.\nWhile RFE necessitates a predetermined number of features to retain, determining the optimal number of features beforehand is often challenging. Cross-validation is employed in conjunction with RFE to evaluate various feature subsets, allowing the selection of the most favorable set based on scores. The RFECV visualizer illustrates the relationship between the number of features in the model, their cross-validated test scores, and variability, ultimately highlighting the optimal number of features chosen.\nRandom Forest Classification tuning with Grid_search:\nA model hyperparameter is an attribute of a model that exists external to the model itself, and its value cannot be deduced from the available data. This value must be predetermined before the commencement of the learning process. Examples include ‘c’ in Support Vector Machines, ‘k’ in k Nearest Neighbors, and the number of hidden layers in Neural Networks.\nIn contrast, a parameter is an inherent characteristic of the model, and its value can be derived from the data. Examples encompass beta coefficients in linear/logistic regression or support vectors in Support Vector Machines.\n\nGrid-search is a technique employed to identify the optimal hyperparameters for a model, aiming to produce the most accurate predictions.\n\nApplying model and found the fitting model with optimized hyperparameters and calculating accuracy:\n# Create a table with the results of the classifiers\n\nscoreTable = pd.DataFrame()\n\ndef add_to_scoreTable(model, accuracy_train, accuracy_test, best_params, scoreTable):\n\n    result = pd.DataFrame(pd.concat([pd.Series({'accuracy_test': accuracy_test, 'accuracy_train': accuracy_train}), pd.Series(best_params)], axis = 0), columns = [model])\n\n    return pd.concat([scoreTable, result], axis = 1)\n\n\n\nclassifier = RandomForestClassifier() \nrfecv = RFECV(estimator=classifier, step=1, cv=5, scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train.values.ravel())\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])\n\nbest_features = x_train.columns[rfecv.support_]\n\n# application of new features to x_train, and y_test\n\nx_train = x_train[best_features]\nx_test = x_test[best_features]\n\nparam_grid = {\n    'n_estimators': [100, 200, 500],\n    'max_features': [ 'sqrt', 'log2'],\n    'max_depth' : [8, 10],\n    'criterion' :['gini', 'entropy']\n    }\n\n# Optimized hyperparameters for Random Forest Classification:\n\ngrid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\n# prediction with opitimized hyperparameters and calculating accuracy\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\nOutputs:\n\n\nConfusion Matrix for Random Forest:\n# Confusion Matrix \n\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('RandomForestClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\nSupport Vector Machine Classification: The goal of the support vector machine algorithm is to identify a hyperplane in an N-dimensional space (where N represents the number of features) that effectively separates and classifies the data points.\nNumerous hyperplanes can be considered to separate the two classes of data points. Our aim is to identify a plane with the maximum margin, signifying the greatest distance between data points of both classes. Maximizing this margin distance enhances the robustness of the classification, allowing for increased confidence in classifying future data points.\n\n\nOptimized hyperparameters for SVC model and calculating accuracy with optimized features:\n# Finding optimized Hyperparameters for SVC model\n\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n    'kernel': ['rbf'] #also tried: 'linear', 'poly', 'sigmoid'\n    }\n\ngrid = GridSearchCV(SVC(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\n# Accuracy for SVC model\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\nOutputs:\n\n\nConfusion Matrix for SVC model:\n# Confusion Matrix of SVC model\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('SVC', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\nOptimized number of features for SVC model:\n\n\nKNN model: The k-nearest neighbors algorithm, commonly referred to as KNN or k-NN, is a non-parametric, supervised learning classifier. It leverages proximity to classify or predict the grouping of an individual data point. Although applicable to both regression and classification problems, it is predominantly employed as a classification algorithm. The underlying assumption is that similar points tend to be located in close proximity to each other.\nImplement KNN model, find optimized n_neighbor and calculating accuracy with optimized parameters:\n# KNN model\naccuracy = np.zeros(100)\nfor i in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors = i).fit(x_train, y_train.values.ravel())\n    yhat= knn.predict(x_test)\n    accuracy[i-1] = accuracy_score(y_test, yhat)\n    \n\n\n# Optimized N for KNN\n\nax = sns.lineplot(accuracy)\nax.set(xlabel = 'n_neighbors', ylabel = 'accuracy')\n\n\nax = sns.lineplot(accuracy[:10])\nax.set(xlabel = 'n_neighbors', ylabel = 'accuracy')\n\nparam_grid = {\n    'n_neighbors' : [i for i in range(1,9)],\n    'weights' : ['uniform','distance'],\n    'metric' : ['minkowski','euclidean','manhattan']\n    }\n\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\n# Accuracy for KNN\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\nOutputs:\n\n\n\nConfusion Matrix for KNN:\n# Confusion Matrix for KNN\n\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('KNeighborsClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\nDecision Tree Classification model:\n# Decision Tree\n\nparam_grid = {\n    'max_features': [ 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n    }\n\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\nOptimized hyperparameters and calculating accuracy:\n# Optimized Hyperparameters and calculating accuracy\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\nOutputs:\n\n\nConfusion Matrix for Decision Tree:\n# Cofusion Matrix for Decision Tree\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('DecisionTreeClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\nResult table for comparison:\n\nAll codes:\n# Importing\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import RFECV\n\n\n\n# loading data\n\ndata = pd.read_csv('fetal_health.csv')\n\n# checking connection between different features\n\ncorr_data = data.copy()\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\n\n\n\n# test and train split\n\ny = data[['fetal_health']]\nfeatures = data.drop(['fetal_health'], axis=1)\nx = (features - features.mean()) / (features.std()) #standardized features\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n# Create a table with the results of the classifiers\n\nscoreTable = pd.DataFrame()\n\ndef add_to_scoreTable(model, accuracy_train, accuracy_test, best_params, scoreTable):\n\n    result = pd.DataFrame(pd.concat([pd.Series({'accuracy_test': accuracy_test, 'accuracy_train': accuracy_train}), pd.Series(best_params)], axis = 0), columns = [model])\n\n    return pd.concat([scoreTable, result], axis = 1)\n\n\n\nclassifier = RandomForestClassifier() \nrfecv = RFECV(estimator=classifier, step=1, cv=5, scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train.values.ravel())\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])\n\nbest_features = x_train.columns[rfecv.support_]\n\n# application of new features to x_train, and y_test\n\nx_train = x_train[best_features]\nx_test = x_test[best_features]\n\nparam_grid = {\n    'n_estimators': [100, 200, 500],\n    'max_features': [ 'sqrt', 'log2'],\n    'max_depth' : [8, 10],\n    'criterion' :['gini', 'entropy']\n    }\n\n# Optimized hyperparameters for Random Forest Classification:\n\ngrid = GridSearchCV(RandomForestClassifier(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\n# prediction with opitimized hyperparameters and calculating accuracy\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\n\n# Confusion Matrix \n\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('RandomForestClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\n# Finding optimized Hyperparameters for SVC model\n\n\nparam_grid = {\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n    'kernel': ['rbf'] #also tried: 'linear', 'poly', 'sigmoid'\n    }\n\ngrid = GridSearchCV(SVC(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\n# Accuracy for SVC model\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\n\n# Confusion Matrix of SVC model\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('SVC', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\n# KNN model\naccuracy = np.zeros(100)\nfor i in range(1,101):\n    knn = KNeighborsClassifier(n_neighbors = i).fit(x_train, y_train.values.ravel())\n    yhat= knn.predict(x_test)\n    accuracy[i-1] = accuracy_score(y_test, yhat)\n    \n\n\n# Optimized N for KNN\n\nax = sns.lineplot(accuracy)\nax.set(xlabel = 'n_neighbors', ylabel = 'accuracy')\n\n\nax = sns.lineplot(accuracy[:10])\nax.set(xlabel = 'n_neighbors', ylabel = 'accuracy')\n\nparam_grid = {\n    'n_neighbors' : [i for i in range(1,9)],\n    'weights' : ['uniform','distance'],\n    'metric' : ['minkowski','euclidean','manhattan']\n    }\n\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\n# Accuracy for KNN\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\n\n# Confusion Matrix for KNN\n\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('KNeighborsClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)\n\n# Decision Tree\n\nparam_grid = {\n    'max_features': [ 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n    }\n\ngrid = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, refit = True, verbose = 3)\ngrid.fit(x_train, y_train.values.ravel())\n\n# Optimized Hyperparameters and calculating accuracy\n\nprint('best parameters: {}\\n'.format(grid.best_params_))\n\ny_pred = grid.best_estimator_.predict(x_test)\n\naccuracy_train = (grid.best_score_ * 100).round(2)\naccuracy_test = (accuracy_score(y_test, y_pred) * 100).round(2)\n\nprint(\"train dataset accuracy : {}%\".format(accuracy_train))\nprint(\"test dataset accuracy : {}%\".format(accuracy_test))\n\n# Cofusion Matrix for Decision Tree\nConfusionMatrixDisplay.from_estimator(grid.best_estimator_, x_test, y_test)\nscoreTable = add_to_scoreTable('DecisionTreeClassifier', accuracy_train, accuracy_test, grid.best_params_, scoreTable)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M. S. Lori",
    "section": "",
    "text": "Outlier or Anomalies, Detection and Removal\n\n\n\n\n\n\nOutlier-Detection\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Theory (Naïve Bayes Classification)\n\n\n\n\n\n\nProbability Theory\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\n\n\n\n\n\n\nKmeans Clustering\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Machine Learning Methods\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\n\n\n\n\n\n\nSpray Cooling Prediction by ML and DL\n\n\n\n\n\n\nPaper\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\n\n\n\n\n\n\nImplement Different Classification Models\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nM. S. Lori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am M. S Lori, a student of mechanical engineering at Virginia Tech. In this blog, I furnish detailed information on various Machine Learning models."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Kmeans Clustering",
    "section": "",
    "text": "In this post, we do a segmentation analysis for the Customer data set\nImporting required libraries:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly as py\nimport plotly.graph_objs as go\nfrom sklearn.cluster import KMeans\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\npy.offline.init_notebook_mode(connected = True)\n#print(os.listdir(\"../input\"))\nLoading the Costumer data set:\ndf = pd.read_csv(r'Mall_Customers.csv')\nVisualization:\nplt.style.use('fivethirtyeight')\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\nplt.figure(1 , figsize = (15 , 5))\nsns.countplot(y = 'Gender' , data = df)\nplt.show()\n\nRelationship between data features\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    for y in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sns.regplot(x = x , y = y , data = df)\n        plt.ylabel(y.split()[0]+' '+y.split()[1] if len(y.split()) &gt; 1 else y )\nplt.show()\n\nCorrelation Matrix:\n\nplt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Age' , y = 'Annual Income (k$)' , data = df[df['Gender'] == gender] ,\n                s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Age'), plt.ylabel('Annual Income (k$)') \nplt.title('Age vs Annual Income w.r.t Gender')\nplt.legend()\nplt.show()\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Annual Income (k$)',y = 'Spending Score (1-100)' ,\n                data = df[df['Gender'] == gender] ,s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)') \nplt.title('Annual Income vs Spending Score w.r.t Gender')\nplt.legend()\nplt.show()\n\n\n\n\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor cols in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1 \n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.violinplot(x = cols , y = 'Gender' , data = df , palette = 'vlag')\n    sns.swarmplot(x = cols , y = 'Gender' , data = df)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Boxplots & Swarmplots' if n == 2 else '')\nplt.show()\n\n\n\nKmeans Clustering\n\nAge and spending score\n\n##Age and spending Score\n\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n    \n    \n    \n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age' ,y = 'Spending Score (1-100)' , data = df , c = labels1 , \n            s = 200 )\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\nAnnual Income and Spending Score\n\n\n##Annual Income and spending Score\n\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n    \n    \n    \n\n    \nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n\n\n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 200 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()\n\nAll Code:\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly as py\nimport plotly.graph_objs as go\nfrom sklearn.cluster import KMeans\nimport warnings\nimport os\nwarnings.filterwarnings(\"ignore\")\npy.offline.init_notebook_mode(connected = True)\n#print(os.listdir(\"../input\"))\n\ndf = pd.read_csv(r'Mall_Customers.csv')\n\nplt.style.use('fivethirtyeight')\n\n\nplt.figure(1 , figsize = (15 , 6))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1\n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace =0.5 , wspace = 0.5)\n    sns.distplot(df[x] , bins = 20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()\n\n\nplt.figure(1 , figsize = (15 , 5))\nsns.countplot(y = 'Gender' , data = df)\nplt.show()\n\n\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor x in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    for y in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n        n += 1\n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n        sns.regplot(x = x , y = y , data = df)\n        plt.ylabel(y.split()[0]+' '+y.split()[1] if len(y.split()) &gt; 1 else y )\nplt.show()\n\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Age' , y = 'Annual Income (k$)' , data = df[df['Gender'] == gender] ,\n                s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Age'), plt.ylabel('Annual Income (k$)') \nplt.title('Age vs Annual Income w.r.t Gender')\nplt.legend()\nplt.show()\n\n\n\nplt.figure(1 , figsize = (15 , 6))\nfor gender in ['Male' , 'Female']:\n    plt.scatter(x = 'Annual Income (k$)',y = 'Spending Score (1-100)' ,\n                data = df[df['Gender'] == gender] ,s = 200 , alpha = 0.5 , label = gender)\nplt.xlabel('Annual Income (k$)'), plt.ylabel('Spending Score (1-100)') \nplt.title('Annual Income vs Spending Score w.r.t Gender')\nplt.legend()\nplt.show()\n\n\n\n\nplt.figure(1 , figsize = (15 , 7))\nn = 0 \nfor cols in ['Age' , 'Annual Income (k$)' , 'Spending Score (1-100)']:\n    n += 1 \n    plt.subplot(1 , 3 , n)\n    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n    sns.violinplot(x = cols , y = 'Gender' , data = df , palette = 'vlag')\n    sns.swarmplot(x = cols , y = 'Gender' , data = df)\n    plt.ylabel('Gender' if n == 1 else '')\n    plt.title('Boxplots & Swarmplots' if n == 2 else '')\nplt.show()\n\n\n\n'''Age and spending Score'''\nX1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X1)\n    inertia.append(algorithm.inertia_)\n    \n    \n    \n\n\nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\nalgorithm = (KMeans(n_clusters = 4 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X1)\nlabels1 = algorithm.labels_\ncentroids1 = algorithm.cluster_centers_\n\n\n\n\nh = 0.02\nx_min, x_max = X1[:, 0].min() - 1, X1[:, 0].max() + 1\ny_min, y_max = X1[:, 1].min() - 1, X1[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = algorithm.predict(np.c_[xx.ravel(), yy.ravel()]) \n\n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ = Z.reshape(xx.shape)\nplt.imshow(Z , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Age' ,y = 'Spending Score (1-100)' , data = df , c = labels1 , \n            s = 200 )\nplt.scatter(x = centroids1[: , 0] , y =  centroids1[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Age')\nplt.show()\n\n\n\n\n'''Annual Income and spending Score'''\nX2 = df[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values\ninertia = []\nfor n in range(1 , 11):\n    algorithm = (KMeans(n_clusters = n ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\n    algorithm.fit(X2)\n    inertia.append(algorithm.inertia_)\n    \n    \n    \n\n    \nplt.figure(1 , figsize = (15 ,6))\nplt.plot(np.arange(1 , 11) , inertia , 'o')\nplt.plot(np.arange(1 , 11) , inertia , '-' , alpha = 0.5)\nplt.xlabel('Number of Clusters') , plt.ylabel('Inertia')\nplt.show()\n\n\n\nalgorithm = (KMeans(n_clusters = 5 ,init='k-means++', n_init = 10 ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='elkan') )\nalgorithm.fit(X2)\nlabels2 = algorithm.labels_\ncentroids2 = algorithm.cluster_centers_\n\n\n\nh = 0.02\nx_min, x_max = X2[:, 0].min() - 1, X2[:, 0].max() + 1\ny_min, y_max = X2[:, 1].min() - 1, X2[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ2 = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n\n\n\n\nplt.figure(1 , figsize = (15 , 7) )\nplt.clf()\nZ2 = Z2.reshape(xx.shape)\nplt.imshow(Z2 , interpolation='nearest', \n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap = plt.cm.Pastel2, aspect = 'auto', origin='lower')\n\nplt.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = df , c = labels2 , \n            s = 200 )\nplt.scatter(x = centroids2[: , 0] , y =  centroids2[: , 1] , s = 300 , c = 'red' , alpha = 0.5)\nplt.ylabel('Spending Score (1-100)') , plt.xlabel('Annual Income (k$)')\nplt.show()"
  },
  {
    "objectID": "posts/Outlier-Detection/index.html",
    "href": "posts/Outlier-Detection/index.html",
    "title": "Outlier or Anomalies, Detection and Removal",
    "section": "",
    "text": "Outliers are values that significantly differ from other observations in a dataset, potentially suggesting variations in measurement, experimental errors, or the presence of something unique. Simply put, an outlier is an observation that deviates from the general pattern within a sample.\nWe have two main outliers type:\nUnivariate: detecting univariate outliers involves examining the distribution of values within a single feature space.\nMultivariate: identifying multivariate outliers occurs within an n-dimensional space corresponding to n features. Analyzing distributions in such multidimensional spaces can be challenging for the human brain, necessitating the use of a trained model to perform this task on our behalf.\nMethods for replacing and removing outliers\n1- Capping: In this approach, we set a threshold for our outlier data, establishing a limit where values above or below a specific threshold are considered outliers. The count of outliers in the dataset corresponds to the number determined by this capping procedure.\n\n2- Trimming: This method involves excluding outlier values from our analysis. Implementing this technique results in a more focused dataset, particularly in situations where there is a notable presence of outliers. Its primary advantage lies in its efficiency, as it is a quick process.\n\n3- Treatment as missing values: By considering outliers as if they were missing observations, treat them in a manner consistent with the treatment of missing values.\n\nLets do some analysis\nImporting necessary libraries:\n #Step-1: Importing Necessary Dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport seaborn as sns\nLoading data:\ndf = pd.read_csv('Placement_data_full_class.csv')\nDo some visualization\nplt.figure(figsize=(16,5))\nplt.subplot(2,3,1)\nsns.distplot(df['degree_p'])\nplt.subplot(2,3,2)\nsns.distplot(df['salary'])\nplt.subplot(2,3,3)\nsns.distplot(df['hsc_p'])\nplt.subplot(2,3,4)\nsns.distplot(df['ssc_p'])\nplt.subplot(2,3,5)\nsns.distplot(df['mba_p'])\nplt.subplot(2,3,6)\nsns.distplot(df['etest_p'])\nplt.show()\n\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\nFinding outliers and plot them:\n\n#Step-4: Finding the Boundary Values\nHighest_allowed_degree=df['degree_p'].mean() + 3*df['degree_p'].std()\nLowest_allowed_degree=df['degree_p'].mean() - 3*df['degree_p'].std()\nHighest_allowed_Salary=df['salary'].mean() + 3*df['salary'].std()\nLowest_allowed_Salary=df['salary'].mean() - 3*df['salary'].std()\n\nprint(\"Highest allowed_degree\",Highest_allowed_degree)\nprint(\"Lowest allowed_degree : \",Lowest_allowed_degree)\nprint(\"Highest allowed Salary\",Highest_allowed_Salary)\nprint(\"Lowest allowed Salary\",Lowest_allowed_Salary)\n\n#Step-5: Finding the Outliers\nOutlier_data=df[(df['salary'] &gt; Highest_allowed_Salary) | (df['salary'] &lt; Lowest_allowed_Salary)].append(df[(df['degree_p'] &gt; Highest_allowed_degree) | (df['degree_p'] &lt; Lowest_allowed_degree)])\n\n\n#plot outliers\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\n1- Capping\nFinding outliers and plotting them:\n## Capping\n\ndf_c=df.copy()\ndf_c['degree_p'][df_c['degree_p']&gt;Highest_allowed_degree]=Highest_allowed_degree\ndf_c['degree_p'][df_c['degree_p']&lt;Lowest_allowed_degree]=Lowest_allowed_degree\n\ndf_c['salary'][df_c['salary']&gt;Highest_allowed_Salary]=Highest_allowed_Salary\ndf_c['salary'][df_c['salary']&lt;Lowest_allowed_Salary]=Lowest_allowed_Salary\n\n\n\n#plot outliers\nplt.scatter(df_c['degree_p'],df_c['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\n2- Trimming\n## Triming\n#just apply pands dataframe filter\ndf_t=df.copy()\ndf_t=df_t[(df_t['salary'] &lt; Highest_allowed_Salary) & (df_t['salary'] &gt; Lowest_allowed_Salary)]\ndf_t=df_t[(df_t['degree_p'] &lt; Highest_allowed_degree) & (df_t['degree_p'] &gt; Lowest_allowed_degree)]\nplt.scatter(df_t['degree_p'],df_t['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\nConsidering outliers as missing values:\n## Considering as missing value\ndf_n=df.copy()\ndf_n['salary'][(df_n['salary'] &gt;= Highest_allowed_Salary) | (df_n['salary'] &lt;= Lowest_allowed_Salary)]=np.nan\ndf_n['degree_p'][(df_n['degree_p'] &gt;= Highest_allowed_degree) | (df_n['degree_p'] &lt;= Lowest_allowed_degree)]=np.nan\nWays for handling outliers\n1- Z-score: the z-score, or standard score, of an observation is a metric indicating how many standard deviations a data point deviates from the mean of the sample, assuming a Gaussian distribution. This renders the z-score a parametric method. Often, data points do not conform to a Gaussian distribution, and this challenge can be addressed by applying transformations to the data, such as scaling. Python libraries like Scipy and Sci-kit Learn offer user-friendly functions and classes for easy implementation, alongside Pandas and Numpy. After appropriately transforming the selected feature space of the dataset, the z-score of any data point can be calculated using the following expression:\nz = (Data Point - Mean / Sd)\nWhen computing the z-score for each sample in the dataset, a threshold must be specified. Commonly used ‘thumb-rule’ thresholds include 2.5, 3, 3.5, or more standard deviations.\nBy ‘tagging’ or removing data points that fall beyond a specified threshold, we categorize the data into outliers and non-outliers. The Z-score method is a straightforward yet potent approach for handling outliers in data, particularly when dealing with parametric distributions in a low-dimensional feature space. In nonparametric scenarios, methods like DBSCAN and Isolation Forests can serve as effective solutions.\nZ-score coding:\n###Z-score\nfrom scipy import stats\nimport pandas as pd\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\nd=pd.DataFrame(stats.zscore(df['salary']),columns=['z_score'])\nd=d[(d['z_score']&gt;3) | (d['z_score']&lt;-3)]\n\n\n\ndegree=[]\nsalary=[]\nfor i in df.index:\n    if( i in d.index): \n        salary.append(df.loc[i]['salary'])\n        degree.append(df.loc[i]['degree_p'])\n        \n\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(degree,salary)\nplt.show()\n\n2- Isolation forest: The nomenclature of this technique derives from its core concept. The algorithm individually examines each data point, categorizing them into outliers or inliers based on the time it takes to isolate them. To elaborate further for clarity: when attempting to isolate a point that is evidently not an outlier, it will be surrounded by many other points, making isolation challenging. Conversely, if the point is an outlier, it stands alone, making isolation relatively straightforward. The detailed explanation of the isolation process will be provided later; your patience is appreciated.\nprocedure:\n1- Select the point to isolate.\n2- For each feature, set the range to isolate between the minimum and the maximum.\n3- Choose a feature randomly.\n4- Pick a value that’s in the range, again randomly\n\nIf the chosen value keeps the point above, switch the minimum of the range of the feature to the value.\nIf the chosen value keeps the point below, switch the maximum of the range of the feature to the value.\n\n5- Repeat steps 3 & 4 until the point is isolated. That is, until the point is the only one which is inside the range for all features.\n6- Count how many times you’ve had to repeat steps 3 & 4. We call this quantity the isolation number.\nThe algorithm asserts that a point is considered an outlier if it doesn’t require iteration through steps 3 and 4 multiple times.\nIt’s important to note that the provided pseudocode is a simplified representation of the actual process for better comprehension. In reality, since the algorithm involves the use of random numbers, this procedure is executed multiple times, and the ultimate isolation score is a composite of all the isolation scores obtained through these iterations.\nCoding:\n###Isolation Forest\n\nfrom sklearn.ensemble import IsolationForest\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing\nmodel=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.05),max_features=1.0)\nmodel.fit(df[['ssc_p']],df[['hsc_p']])\n\ndg=pd.DataFrame({'ssc_p':df['ssc_p'],\n                 'score':model.decision_function(df[['ssc_p']]),\n                 'anomaly':model.predict(df[['ssc_p']]),\n                 'hsc_p':df['hsc_p']})\nimport matplotlib.pyplot as plt\ndg2=dg[dg['anomaly']==-1]\ndg2\n\nplt.scatter(dg['ssc_p'],dg['hsc_p'])\nplt.scatter(dg2['ssc_p'],dg2['hsc_p'])\nplt.show()\n\n\n3- DBScane Anomaly Detection: As its name implies, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based and unsupervised machine learning algorithm. It processes multi-dimensional data, clustering them based on model parameters such as epsilon and minimum samples. Using these parameters, the algorithm decides whether specific values in the dataset qualify as outliers.\nDBSCAN consolidates densely grouped data points into a single cluster. It excels in identifying clusters within large spatial datasets by assessing the local density of data points. Notably, DBSCAN exhibits robustness to outliers. Unlike K-Means, where the number of centroids must be predetermined, DBSCAN does not require the a priori specification of the number of clusters.\nDBSCAN relies on two key parameters: epsilon and minPoints. Epsilon represents the radius of the circle formed around each data point to assess density, and minPoints denotes the minimum number of data points needed within that circle for the data point to be labeled as a Core point.\nIn higher dimensions, the circle transforms into a hypersphere, where epsilon becomes the radius of the hypersphere, and minPoints remains the minimum required number of data points within that hypersphere.\nHere, we have a set of data points depicted in grey. Let’s observe how DBSCAN organizes and groups these data points into clusters.\n\nDBSCAN forms a circle with an epsilon radius around each data point and categorizes them into Core points, Border points, and Noise. A data point is identified as a Core point if the circle around it encompasses at least the specified ‘minPoints’ number of points. If the count of points is below the minPoints threshold, it is labeled as a Border Point. In cases where there are no other data points within the epsilon radius of a particular data point, it is designated as Noise.\n\nThe depicted illustration illustrates a cluster generated by DBSCAN with a specified minPoints value of 3. In this context, a circle with an equal radius (epsilon) is drawn around each data point. These two parameters play a crucial role in forming spatial clusters.\nData points with a minimum of 3 points, including the point itself, within the circle are designated as Core points, distinguished by the color red. Data points with fewer than 3 but more than 1 point, including itself, within the circle are categorized as Border points, represented by the color yellow. Lastly, data points with no other points aside from itself within the circle are identified as Noise, depicted by the color purple.\nCoding:\n## DBScane Anomaly Detection\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)] #reindexing | change accordingle to reset index of df\n\n\n#before DBSCAN you must scale your dataset\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\n\ndbsc = DBSCAN(eps = 1.3, min_samples = 25).fit(df)\nlabels = dbsc.labels_\noutliers=df[dbsc.labels_==-1]\n\nplt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"salary\")\nplt.show()\n\nAll codes:\n\n\n#Step-1: Importing Necessary Dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport seaborn as sns\n\n#Step-2: Read and Load the Dataset\ndf = pd.read_csv('Placement_data_full_class.csv')\n\n\n\nplt.figure(figsize=(16,5))\nplt.subplot(2,3,1)\nsns.distplot(df['degree_p'])\nplt.subplot(2,3,2)\nsns.distplot(df['salary'])\nplt.subplot(2,3,3)\nsns.distplot(df['hsc_p'])\nplt.subplot(2,3,4)\nsns.distplot(df['ssc_p'])\nplt.subplot(2,3,5)\nsns.distplot(df['mba_p'])\nplt.subplot(2,3,6)\nsns.distplot(df['etest_p'])\nplt.show()\n\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\n\n\n#Step-4: Finding the Boundary Values\nHighest_allowed_degree=df['degree_p'].mean() + 3*df['degree_p'].std()\nLowest_allowed_degree=df['degree_p'].mean() - 3*df['degree_p'].std()\nHighest_allowed_Salary=df['salary'].mean() + 3*df['salary'].std()\nLowest_allowed_Salary=df['salary'].mean() - 3*df['salary'].std()\n\nprint(\"Highest allowed_degree\",Highest_allowed_degree)\nprint(\"Lowest allowed_degree : \",Lowest_allowed_degree)\nprint(\"Highest allowed Salary\",Highest_allowed_Salary)\nprint(\"Lowest allowed Salary\",Lowest_allowed_Salary)\n\n#Step-5: Finding the Outliers\nOutlier_data=df[(df['salary'] &gt; Highest_allowed_Salary) | (df['salary'] &lt; Lowest_allowed_Salary)].append(df[(df['degree_p'] &gt; Highest_allowed_degree) | (df['degree_p'] &lt; Lowest_allowed_degree)])\n\n\n#plot outliers\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n## Capping\n\ndf_c=df.copy()\ndf_c['degree_p'][df_c['degree_p']&gt;Highest_allowed_degree]=Highest_allowed_degree\ndf_c['degree_p'][df_c['degree_p']&lt;Lowest_allowed_degree]=Lowest_allowed_degree\n\ndf_c['salary'][df_c['salary']&gt;Highest_allowed_Salary]=Highest_allowed_Salary\ndf_c['salary'][df_c['salary']&lt;Lowest_allowed_Salary]=Lowest_allowed_Salary\n\n\n\n#plot outliers\nplt.scatter(df_c['degree_p'],df_c['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\n## Triming\n#just apply pands dataframe filter\ndf_t=df.copy()\ndf_t=df_t[(df_t['salary'] &lt; Highest_allowed_Salary) & (df_t['salary'] &gt; Lowest_allowed_Salary)]\ndf_t=df_t[(df_t['degree_p'] &lt; Highest_allowed_degree) & (df_t['degree_p'] &gt; Lowest_allowed_degree)]\nplt.scatter(df_t['degree_p'],df_t['salary'])\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(Outlier_data['degree_p'],Outlier_data['salary'])\n#changel lim according to your data\nplt.xlim(0,100)\nplt.ylim(0,1000000)\nplt.show()\n\n\n## Considering as missing value\ndf_n=df.copy()\ndf_n['salary'][(df_n['salary'] &gt;= Highest_allowed_Salary) | (df_n['salary'] &lt;= Lowest_allowed_Salary)]=np.nan\ndf_n['degree_p'][(df_n['degree_p'] &gt;= Highest_allowed_degree) | (df_n['degree_p'] &lt;= Lowest_allowed_degree)]=np.nan\n\n\n###Z-score\nfrom scipy import stats\nimport pandas as pd\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df\nd=pd.DataFrame(stats.zscore(df['salary']),columns=['z_score'])\nd=d[(d['z_score']&gt;3) | (d['z_score']&lt;-3)]\n\n\n\ndegree=[]\nsalary=[]\nfor i in df.index:\n    if( i in d.index): \n        salary.append(df.loc[i]['salary'])\n        degree.append(df.loc[i]['degree_p'])\n        \n\nimport matplotlib.pyplot as plt\nplt.scatter(df['degree_p'],df['salary'])\nplt.scatter(degree,salary)\nplt.show()\n\n\n###Isolation Forest\n\nfrom sklearn.ensemble import IsolationForest\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()\ndf.index=[i for i in range(0,148)]#reindexing\nmodel=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.05),max_features=1.0)\nmodel.fit(df[['ssc_p']],df[['hsc_p']])\n\ndg=pd.DataFrame({'ssc_p':df['ssc_p'],\n                 'score':model.decision_function(df[['ssc_p']]),\n                 'anomaly':model.predict(df[['ssc_p']]),\n                 'hsc_p':df['hsc_p']})\nimport matplotlib.pyplot as plt\ndg2=dg[dg['anomaly']==-1]\ndg2\n\nplt.scatter(dg['ssc_p'],dg['hsc_p'])\nplt.scatter(dg2['ssc_p'],dg2['hsc_p'])\nplt.show()\n\n\n## DBScane Anomaly Detection\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)] #reindexing | change accordingle to reset index of df\n\n\n#before DBSCAN you must scale your dataset\nstscaler = StandardScaler().fit(df)\ndf = pd.DataFrame(stscaler.transform(df))\n\ndbsc = DBSCAN(eps = 1.3, min_samples = 25).fit(df)\nlabels = dbsc.labels_\noutliers=df[dbsc.labels_==-1]\n\nplt.scatter(df[0],df[1])\nplt.scatter(outliers[0],outliers[1])\nplt.xlabel(\"Degree_p\")\nplt.ylabel(\"salary\")\nplt.show()\n\n\n### Elliptic Envelope\n\nfrom sklearn.covariance import EllipticEnvelope\nimport pandas as pd\ndf = pd.read_csv('Placement_data_full_class.csv')\ndf=df.dropna()[['degree_p','salary']]\ndf.index=[i for i in range(0,148)]#reindexing | change accordingle to reset index of df"
  },
  {
    "objectID": "posts/regression-machine-learning-methods/index.html",
    "href": "posts/regression-machine-learning-methods/index.html",
    "title": "Regression Machine Learning Methods",
    "section": "",
    "text": "In this post, we use some regression machine learning methods on a dataset and make a comparison of outcomes for different methods.  The applying methods are 1- Random Forest Regression 2- Support Vector Regression (SVR) 3- Multiple Linear Regression and 4- Linear Regression\n1- Random Forest Regression is a supervised learning algorithm and bagging technique within machine learning that utilizes an ensemble learning method for regression. The trees in Random Forests run in parallel, implying no interaction between these trees during their construction.\nRandom Forest Regression is a supervised learning algorithm employing an ensemble learning approach for regression.\nIt’s important to note that Random Forest is a bagging technique rather than a boosting technique. The trees in Random Forests operate in parallel, indicating that there is no interaction between these trees during the tree-building process.\n\n2- Support Vector Regression (SVR): Support Vector Regression (SVR) represents a machine learning algorithm employed in regression analysis. Its objective is to identify a function that estimates the connection between input variables and a continuous target variable, with a focus on minimizing prediction errors.\nIn contrast to Support Vector Machines (SVMs) utilized for classification purposes, SVR aims to locate a hyperplane that optimally accommodates data points in a continuous space. This involves mapping input variables to a high-dimensional feature space and determining the hyperplane that maximizes the margin (distance) from the hyperplane to the nearest data points, all while minimizing prediction errors.\nSVR exhibits the capability to address non-linear associations between input variables and the target variable through the incorporation of a kernel function, facilitating the mapping of data to a higher-dimensional space. This characteristic renders SVR particularly potent for regression tasks characterized by intricate relationships between input variables and the target variable.\nHyperparameters of the Support Vector Regression (SVR) and Support Vector Machine (SVM) Algorithms:\n\nKernel: Utilizing a kernel aids in identifying a hyperplane within a higher-dimensional space without escalating the computational cost. Typically, the computational cost rises as the dimension of the data increases. This escalation in dimensionality becomes necessary when a separating hyperplane cannot be found within the given dimension, compelling the need to transition to a higher-dimensional space.\nHyperplane:In SVM, this essentially represents a demarcating line between two classes of data. However, in Support Vector Regression, this line serves as the basis for predicting continuous output values.\nDecision Boundary: A decision boundary can be conceptualized as a demarcation line (for simplicity), with positive examples on one side and negative examples on the other. Along this line, examples may be classified as either positive or negative. This identical concept from SVM is also employed in Support Vector Regression.\n\nThere are many hyperplanes to separate the data, but in SVR, we look for a hyperplane with the maximum margin (distance) between two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. (for support vector machine)\nEnvision the two blue lines as the decision boundary, with the green line serving as the hyperplane. In the context of SVR, our aim is primarily focused on the points situated within the decision boundary line. The optimal fit line corresponds to the hyperplane that encompasses the maximum number of points.\n\n3&4- Multiple Linear Regression and Linear Regression: Linear regression referred to as simple regression, linear regression establishes a connection between two variables. Graphically, linear regression is represented by a straight line, where the slope defines how changes in one variable influence changes in the other. The y-intercept in a linear regression relationship signifies the value of one variable when the other variable is 0. In instances of intricate connections within data, the relationship may be elucidated by more than a single variable. In such cases, analysts employ multiple regression, a technique aimed at elucidating a dependent variable by considering multiple independent variables.\nAfter getting familiarized with different machine learning regression model, we will start to use those models on real life data.\nImporting our needed libraries:\n## Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nFixing column names:\n## Importing the dataset\n\ndata = pd.read_csv(\"Life Expectancy Data.csv\")\n\n\n## Correcting column names\n\ncolumn = data.columns.to_list()\n\n\ndata.columns = [a.strip() for a in column]\nStrip (): Remove spaces at the beginning and at the end of the string\nHandling missing data:\n## Taking care of missing data\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\ndata['Life expectancy'] = imputer.fit_transform(data[['Life expectancy']])\ndata['Adult Mortality'] = imputer.fit_transform(data[['Adult Mortality']])\ndata['Alcohol'] = imputer.fit_transform(data[['Alcohol']])\ndata['Hepatitis B'] = imputer.fit_transform(data[['Hepatitis B']])\ndata['BMI'] = imputer.fit_transform(data[['BMI']])\ndata['Polio'] = imputer.fit_transform(data[['Polio']])\ndata['Total expenditure'] = imputer.fit_transform(data[['Total expenditure']])\ndata['Diphtheria'] = imputer.fit_transform(data[['Diphtheria']])\ndata['GDP'] = imputer.fit_transform(data[['GDP']])\ndata['Population'] = imputer.fit_transform(data[['Population']])\ndata['thinness  1-19 years'] = imputer.fit_transform(data[['thinness  1-19 years']])\ndata['thinness 5-9 years'] = imputer.fit_transform(data[['thinness 5-9 years']])\ndata['Income composition of resources'] = imputer.fit_transform(data[['Income composition of resources']])\ndata['Schooling'] = imputer.fit_transform(data[['Schooling']])\nSklearn_impute_SimpleImputer: Utilizing a univariate imputer to fill in missing values through straightforward strategies. This involves replacing missing values within each column using a descriptive statistic like mean, median, or the most frequent value, or alternatively, using a constant value.\nEncoding of dependent variables:\n## Encoding of dependent variables\ncountry_dummy = pd.get_dummies(data['Country'], drop_first = True)\nstatus_dummy = pd.get_dummies(data['Status'], drop_first = True)\n\ndata.drop(['Country', 'Status'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\nget_dummies: Convert categorical variable into dummy/indicator variables.  Each variable is converted in as many 0/1 variables as there are different values. Columns in the output are each named after a value; if the input is a DataFrame, the name of the original variable is prepended to the value\nA correlation matrix is a statistical tool employed to assess the relationship between two variables within a dataset. Represented as a table, each cell in the matrix contains a correlation coefficient. A coefficient of 1 signifies a strong positive relationship between variables, 0 indicates a neutral relationship, and -1 suggests a strong negative relationship.\n## Correlation matrix\ncorrelation_matrix = corr_data.corr()\n\nplt.figure(figsize=(16,12))\nsns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\nplt.show()\nHere we have the correlation matrix for our data:\n\nSplit arrays or matrices into random train and test subsets with test size of 20%:\n## Splitting the dataset into the Training set and Test set\nY = pd.DataFrame(data['Life expectancy'], columns = ['Life expectancy'])\ndata.drop(['Life expectancy'], inplace = True, axis = 1)\n\nall_data = pd.concat([data, country_dummy, status_dummy], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(all_data, Y, test_size = 0.2, random_state = 0)\n\ny_test.columns = ['y_test']\nImplement, prediction and evaluation of Random Forest Regression:\n## Random Forest Regression model\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train.values.ravel())\n\n#Predicting the Test set results\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n\n# Evaluating the Model Performance\nr2_random_forest = r2_score(y_test, y_pred)\nImplement, prediction and evaluation of Support Vector Regression (SVR):\n##Support Vector Regression (SVR) model\n# Feature scaling\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train_scaled = sc_X.fit_transform(X_train)\ny_train_scaled = sc_y.fit_transform(y_train)\n\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train_scaled, y_train_scaled.ravel())\n\n##Predicting the Test set results\ny_pred = regressor.predict(sc_X.transform(X_test))\ny_pred = pd.DataFrame(sc_y.inverse_transform(pd.DataFrame(y_pred)), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nsvr_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_svm = r2_score(y_test, y_pred)\nImplement, prediction and evaluation of Multiple Linear Regression:\n##Multiple Linear Regression model\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\n\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_multiple_linear_regression = r2_score(y_test, y_pred)\nImplement, prediction and evaluation of Decision Tree Regression:\n## Decision Tree Regression model\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\n\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_decision_tree_regression = r2_score(y_test, y_pred)\nImplement, prediction and evaluation of Linear Regression:\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n# Set up and fit the linear regressor\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Flatten the prediction and expected lists\npredicted = flatten(lin_reg.predict(X_test))\nexpected = flatten(y_test.values)\n\nimport plotly.express as px\n\n# Put data to plot in dataframe\ndf_plot = pd.DataFrame({'expected':expected, 'predicted':predicted})\n\n# Make scatter plot from data\nfig = px.scatter(\n    df_plot, \n    x='expected', \n    y='predicted',\n    title='Predicted vs. Actual Values')\n\n# Add straight line indicating perfect model\nfig.add_shape(type=\"line\",\n    x0=30, y0=30, x1=90, y1=90,\n    line=dict(\n        color=\"Red\",\n        width=4,\n        dash=\"dot\",\n    )\n)\n\n# Show figure\nfig.show()\nerror = np.sqrt(np.mean((np.array(predicted) - np.array(expected)) ** 2))\nprint(f\"RMS: {error:.4f} \")\n\nr2=r2_score(expected, predicted)\nprint(f\"R2: {round(r2,4)}\") \nComparison of model results:\n##Comparison of model results\nresults = pd.DataFrame([[\n    r2_random_forest,\n    r2_svm, \n    r2_multiple_linear_regression,\n    r2_decision_tree_regression,\nr2]], columns = ['RandomForest', 'SVM', 'Multiplelinear', 'DecisionTree ', ' LinearRegresion'])\n\nax = sns.barplot(results, width = 0.5)\nax.set(xlabel='Model', ylabel='R2 Score')\nplt.ylim(0.9, 1)\nAs you can see, our models have a good prediction (R_square close to 1)\n\n\n\nAll codes:\n## Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\n## Importing the dataset\n\ndata = pd.read_csv(\"Life Expectancy Data.csv\")\n\n\n## Correcting column names\n\ncolumn = data.columns.to_list()\n\n\ndata.columns = [a.strip() for a in column]\n\n## Taking care of missing data\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\ndata['Life expectancy'] = imputer.fit_transform(data[['Life expectancy']])\ndata['Adult Mortality'] = imputer.fit_transform(data[['Adult Mortality']])\ndata['Alcohol'] = imputer.fit_transform(data[['Alcohol']])\ndata['Hepatitis B'] = imputer.fit_transform(data[['Hepatitis B']])\ndata['BMI'] = imputer.fit_transform(data[['BMI']])\ndata['Polio'] = imputer.fit_transform(data[['Polio']])\ndata['Total expenditure'] = imputer.fit_transform(data[['Total expenditure']])\ndata['Diphtheria'] = imputer.fit_transform(data[['Diphtheria']])\ndata['GDP'] = imputer.fit_transform(data[['GDP']])\ndata['Population'] = imputer.fit_transform(data[['Population']])\ndata['thinness  1-19 years'] = imputer.fit_transform(data[['thinness  1-19 years']])\ndata['thinness 5-9 years'] = imputer.fit_transform(data[['thinness 5-9 years']])\ndata['Income composition of resources'] = imputer.fit_transform(data[['Income composition of resources']])\ndata['Schooling'] = imputer.fit_transform(data[['Schooling']])\n\n## Encoding of dependent variables\ncountry_dummy = pd.get_dummies(data['Country'], drop_first = True)\nstatus_dummy = pd.get_dummies(data['Status'], drop_first = True)\n\ndata.drop(['Country', 'Status'], inplace = True, axis = 1)\n\ncorr_data = data.copy()\n\n## Correlation matrix\n##correlation_matrix = corr_data.corr()\n\n##plt.figure(figsize=(16,12))\n##sns.heatmap(correlation_matrix, annot=True,fmt=\".2f\", cmap=\"Greens\")\n##plt.show()\n\n## Splitting the dataset into the Training set and Test set\nY = pd.DataFrame(data['Life expectancy'], columns = ['Life expectancy'])\ndata.drop(['Life expectancy'], inplace = True, axis = 1)\n\nall_data = pd.concat([data, country_dummy, status_dummy], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(all_data, Y, test_size = 0.2, random_state = 0)\n\ny_test.columns = ['y_test']\n\n## Random Forest Regression model\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X_train, y_train.values.ravel())\n\n#Predicting the Test set results\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n\n# Evaluating the Model Performance\nr2_random_forest = r2_score(y_test, y_pred)\n\n\n##Support Vector Regression (SVR) model\n# Feature scaling\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_train_scaled = sc_X.fit_transform(X_train)\ny_train_scaled = sc_y.fit_transform(y_train)\n\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train_scaled, y_train_scaled.ravel())\n\n##Predicting the Test set results\ny_pred = regressor.predict(sc_X.transform(X_test))\ny_pred = pd.DataFrame(sc_y.inverse_transform(pd.DataFrame(y_pred)), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nsvr_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_svm = r2_score(y_test, y_pred)\n\n##Multiple Linear Regression model\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\n\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_multiple_linear_regression = r2_score(y_test, y_pred)\n\n## Decision Tree Regression model\nregressor = DecisionTreeRegressor(random_state = 0)\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\n\ny_pred = pd.DataFrame(regressor.predict(X_test), columns= ['y_pred'])\n\ny_test.index = y_pred.index\n\nrandom_forest_result = pd.concat([y_pred, y_test], axis = 1)\n\n# Evaluating the Model Performance\n\nr2_decision_tree_regression = r2_score(y_test, y_pred)\n\n##Comparison of model results\nresults = pd.DataFrame([[\n    r2_random_forest,\n    r2_svm, \n    r2_multiple_linear_regression,\n    r2_decision_tree_regression,\nr2]], columns = ['RandomForest', 'SVM', 'Multiplelinear', 'DecisionTree ', ' LinearRegresion'])\n\nax = sns.barplot(results, width = 0.5)\nax.set(xlabel='Model', ylabel='R2 Score')\nplt.ylim(0.9, 1)\n\n\n\n\n\n## me\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n# Set up and fit the linear regressor\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\n# Flatten the prediction and expected lists\npredicted = flatten(lin_reg.predict(X_test))\nexpected = flatten(y_test.values)\n\nimport plotly.express as px\n\n# Put data to plot in dataframe\ndf_plot = pd.DataFrame({'expected':expected, 'predicted':predicted})\n\n# Make scatter plot from data\nfig = px.scatter(\n    df_plot, \n    x='expected', \n    y='predicted',\n    title='Predicted vs. Actual Values')\n\n# Add straight line indicating perfect model\nfig.add_shape(type=\"line\",\n    x0=30, y0=30, x1=90, y1=90,\n    line=dict(\n        color=\"Red\",\n        width=4,\n        dash=\"dot\",\n    )\n)\n\n# Show figure\nfig.show()\nerror = np.sqrt(np.mean((np.array(predicted) - np.array(expected)) ** 2))\nprint(f\"RMS: {error:.4f} \")\n\nr2=r2_score(expected, predicted)\nprint(f\"R2: {round(r2,4)}\")"
  }
]